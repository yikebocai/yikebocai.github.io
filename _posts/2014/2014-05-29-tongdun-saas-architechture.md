---
layout: post
title: 反欺诈SAAS服务技术架构
categories: tongdun
tags: 
- arch
---

>经过三个多月的研发和试用，我们的反欺诈SAAS服务已正式在线上运行，具备了基本的分布式系统的雏形，虽说之前对这一套挺熟悉的，但那是建立在阿里强大的基本平台和兄弟部门支撑的基础上，对创业公司来说从头构建一套系统不是那么容易的，正所谓知易行难，关键在细节。

![tongdun saas arch](http://yikebocai.com/myimg/tongdun-saas-arch.png)

## 服务器和虚拟化

硬件可供选择的余地不是很大，DELL、IBM用的还是比较多，前者的性价比更高一点，刚好有人介绍了一家DELL在杭州的经销商，价格还可以就选用了DELL的服务器，最近又加了好多台服务器都是这家的，后面要再找供应商了解下，货比三家么。

应用服务器使用了两颗的`Xeon`四核处理器，内存是16G，硬盘选用了`SSD`硬盘，相对传统机械硬盘`SSD`对IO密集形的服务来说性能提升明显，但现在想来其实应用服务器读写磁盘的时候很少，是没必要用`SSD`硬盘的，倒是数据库应该使用，阿里的MySQL服务器近两年基本都升级到了`SSD`，这对性能的提升还是非常明显的。数据库是应用的核心系统，因此使用了2U的服务器，内存加大到了32G，但大容量的`SSD`硬盘的价格实在太贵，退而求期次使用了15000转的`SAS`硬盘，做了性价比最高的`RAID5`方案，这样保证在一块硬盘坏掉时数据不丢失。缓存服务器使用了48G大内存小硬盘的配置，应付短期内的业务需求应该没什么问题。

根据之前在阿里的经验，应用服务器一般都是用`Xen`做虚拟化，但我们自己在这方面缺少经验，因此还是根据供应商的建议使用`VMWare`的虚拟化方案，因为一开始经验不足，应用服务器的选型也不太合理，两颗四核CPU十六G内存只能虚两台出来，每台分8G有点浪费，虚四台CPU又有点弱，关键是1U的服务器只虚两台也占用一个机位搬托管费用一年就是2800块，太不划算了，后来都干脆买了更高的配置，多虚几台出来。

但在每台物理机上虚N台的方式不能统一管理服务器资源缺乏弹性，在我的理想中应该是可以把一堆物理统一管理起来，然后在上面做虚机，底层物理机的资料管理维护和上层虚拟机相分离，接近私有云的概念。大概了解了最近非常火的`OpenStack`和`CloudStack`解决方案，感觉都还比较复杂，不是我们初创公司能玩的起的，但我觉得今后反欺诈的SAAS服务肯定应该构建在私有云的基础上，最近在找运维人员时特别留意是否有关注这块的人才。

## 负载均衡和集群

因为我们提供的是SAAS服务，目标是为大量的互联网企业用户提供服务，因此这个系统一开始就要求是基于互联网架构的分布式系统，对高可用性、高可扩展性、高并发性都有非常高的要求。

### Apache代理

最前端是用`Apache`做了反向代理，这台机器绑定电信和联通的双IP，分配了10M的独享带宽。之前很长一段时间老是搞不清楚反向代理是什么意思，后来和正向代理做了对比才弄清楚，原来像局域网代理上网，是内网机器通过一台代理服务器访问互联网叫正向代理，而外面的用户通过`Apache`或`Nginx`访问内部实际的应用服务器和它正相反，所以叫反向代理。在Apache的配置中启用了`mod_proxy`和`mod_proxy_balance`模块，使用了反向代理和负载匀衡的功能。负载匀衡算法常见的有`轮询`、`随机`、`权重`等，Apache这里默认的是随机算法。Apache和后面的Tomcat应用服务器，通过`HTTP`的方式实现连接，当然也可以使用更高效的`AJP`协议，它是二进制的传输效率相对来高一点。当初只所以没有用非常流行的`Nginx`，主要还是没人熟悉，而Apache相对了解一点，相对而言快速开发上线接入客户比更完美的技术架构更重要。

但是所有的入口都走Apache的话，就存在单点的问题，万一Apache这台机器挂掉后，后面所有的服务都将无法访问，这对提供SAAS服务的网站来说绝对无法容忍，因此有了`Keepalived`和`Heartbeat`这样的热备方案，主机叫Master，备机叫Slave，主备之前通过心跳来检测状态是否正常，一但发现主机没有反映，将自动启用备机。当然，当主机恢复正常时，备机将再次交出控制权退居幕后。实现的机制除了心跳检测外，最重要的是IP的漂移技术。正常情况下，服务器上启用两个网上`eth0`和`eth1`，分别配置内网和公网IP，如果要实现热备就要求自动把公网IP绑定到备机上，这里就用到[Virtual IP](http://en.wikipedia.org/wiki/Virtual_IP_address)的概念。实际上公网IP并不是想像的那样，是被`Keepalived`自动修改网卡配置然后重启后网卡后生效的，而是请求方通过`ARP`协议在网络中查找该IP对应的`Mac`地址是多少，正常时主机告诉请求方自己的`Mac`地址，切换由后备机响应，这样请求就能发送到真实的服务器上，从而实现了`Failover`。对VIP原理的解释请参考这篇文章[虚拟IP切换原理](http://blog.csdn.net/zhang9981204/article/details/6316333)。

### Tomcat应用

Apache后面是应用集群，应用服务器采用的是轻量级的`Tomcat`，选用它的原因也是因为有广泛的群众基础，目前阶段在技术选型上会偏保守，尽量随大流，即使出现什么问题，也能快速在互联网上找到相应的解决方案，而性能及其它上面的一点点损失在早期是完全可以接受的。

Web应用框架使用了阿里的`Webx`，一个类似`Struts`、`SpringMVC`和MVC框架，选用它的主要原因也是因为熟悉可以快速上手，并且经过在阿里大规模的使用，各方面的功能都经受住了考验，尤其是安全性方面。`Spring`框架也是必不可少的，也几乎成了Java应用程序的标配。数据层使用了`MyBatis`来替代了之前的`IBatis`，对开发来说最大的好处就是Dao实现类的代码可以省掉，只要定义好Dao的接口和SQLMap文件就好了，MyBatis会自动实现，这对开发者来说确实节省了很多人力资源，原来每个Dao类都要在搞一个实现类，纯体力劳动。分页功能可以通过拦截器来实现，避免了在SQLMap中写死，优雅简洁了很多。

因为SAAS服务是一个同步调用服务，背后又涉及复杂的数据统计分析计算，因此并发性能是个很大的挑战，要求尽可能快的完成数据分析和计算并返回风险评估结果，是实时计算的一个典型应用。业务逻辑处理主要包括数据的接收和预处理、规则引擎执行、Velocity计算、数据存储等，其中前三步是每次做风险评估所必须的，需要同步处理，后一步是为了下次数据计算，或者用于用户查询和生成报表，因此把前三步放在同步逻辑中，最后一步放在异步逻辑里，不影响最终评估结果的步骤统统去除。

为了避免高并发时数据存储部分处理慢导致事件堆积过多，也借鉴了之前淘宝风控系统`MTee`的作法，引入了本地队列的机制，所有要异步处理的数据都先写到用`Berkeley DB`实现的本地队列中，然后由多个`worker`线程慢慢处理，这样的结构可以支撑很高的并发量。后续再专门写写服务端的性能优化过程，中间还是有很多有意思的点。

## Memcache缓存

为了提高数据计算的速度，配置了一个大内存的缓存服务器，所有的Velocity计算都先从缓存中查，查不到再从DB中查，线下测试增加缓存后速度可以至少提升5倍以上。一个大型网站之所以能在支撑海量用户的情况下还能做到速度飞快，主要原因就是用到了大量的缓存，从前端的静态资源文件CDN缓存，到静态页面缓存，到用户、交易、商品等各种信息的缓存，可以说缓存无处不在，像阿里在双11时，基本上要将大部分流量都要引导到缓存上，分布式数据库再牛B也无法支撑过高的并发。

##MySQL数据库

数据库采用了MySQL，简单灵活又免费，有成熟的主从复制和热备方案，并且将来数据量大了也有成熟的分布式解决方案。但我们的业务特点决定，对存储需求最大的用户行为数据和Velocity并不太适合MySQL，NoSQL也许是更好的选择，MySQL只存储用户信息、业务规则及系统配置等信息，这些信息数据量不大并且不是KV结构能够方便表示的，传统数据库更适合。一但把用户行为数据迁移出去，我预计在一两年之内一主一备的架构将完全能满足业务的快速发展。

## 系统运维和发布

在线上部署了三套环境，分别是`sandbox沙箱环境`、`staging预发布环境`和`online正式环境`，沙箱环境专门为客户接入提供功能测试，预发布环境用来验证线上发布功能等各方面是否正常，但客户是看不到的也不影响客户的使用，只不过是和正式环境共用同一套数据库，正式环境是真正在线为客户提供服务的系统。

每个应用程序都按标准的结构创建，其中`deploy`部分是程序打包和启动关闭的核心，编译时使用了Spring的resource插件对配置项进行替换，目前看来基本满足要求，但没有之前阿里的`autoconfig`方便，后续考虑要使用它来做些改造。deploy还包括应用的启动和关闭脚本，主要是检查Java、Tomcat等环境是否正常，调用Tomcat脚本启动Web应用，调用一个检测URL确定应用是否正常启动，启动成功后会生成一PID的文件，需要关闭时直接读取该PID文件得到应用程序的进程号，直接Kill掉即可，这样一个好处就是可以在一台服务器上方便地部署多个应用，这对测试等环境特别有用。

应用程序本身支持通过命令来启动和关闭后，就是线上应用程序的发布了。据说`Python`是写运维脚本的利器，但之前没用过，还是用更加熟悉的`Shell`脚本写了一套自动化发布程序，每次发布时只要选择要发布的应用，发布到哪个环境，发布程序就会自动更新主干代码、编译打包、分发到应用服务器、重启应用并备份发布包，目前使用的还挺好。

但也有一点不尽人意的是，在每次发布时，直接调用应用程序的shutdown脚本直接关闭，这可能会影响没有处理完毕的数据，Apache本身就不支持调用命令或接口来实现负载机器的动态调整，据说Nginx是支持，后面要么切换到Nginx，要么在应用程序上做一些处理，每次重启前先调用接口通知它停止服务并将数据刷新到硬盘中，完成优雅重启尽量减少对业务的影响。

线上服务器难免会出现当机、应用崩溃或业务逻辑异常等问题，因此监控系统是必不可少的，业务流行的`Nagios+Catti`、`Zabbix`等，我们选的是后者，有强大的功能基本可以满足我们的一切需求，不过就是界面太老土了，完全和互联网的风格不搭边。Zabbix需要在每台应用服务器上安装Agent，用来收集数据，然后还需要一个Server来统计数据和提供Web管理，我们主要用到了硬件信息的监控，比如Load、内存和磁盘信息，应用本身是否正常的监控，应用日志异常信息的监控等，可以自定义Trigger和Action来实现邮件报警。

>目前的架构，尽量使用成熟但可能不是最优的解决方案，先有再优，后续还有很多需要优化的地方，比如应用的优雅切换、数据存储的优化，包括Velocity的计算优化等。技术是为用户服务的，这点技术人员要牢记，切不可为了技术而技术。

